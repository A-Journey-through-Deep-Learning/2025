{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2-p_EZYaY02"
   },
   "source": [
    "# Lab 1: Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u67Q_Mv2aY1r"
   },
   "source": [
    "## 1.1 Refresh: build a Single Layer Perceptron\n",
    "Let's build a single layer perceptron composed by one dense layer.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGyGeFxcaY1y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the number of inputs \n",
    "n_input_nodes = 2\n",
    "\n",
    "\n",
    "# First define the model \n",
    "model = Sequential() # model lets us define a linear stack of network layers.\n",
    "\n",
    "# define our single fully connected network layer\n",
    "dense_layer = Dense(n_input_nodes, activation='sigmoid',kernel_initializer=\"Ones\",bias_initializer=\"Ones\")\n",
    "\n",
    "# Add the dense layer to the model\n",
    "model.add(dense_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7O3ScCB-aY19"
   },
   "source": [
    "## 1.2 Refresh: build a Multilayer perceptron\n",
    "Let's build a multilayer perceptron; MLPs are fully connected, each node in one layer connects with a certain weight to every node in the following layer.\n",
    "\n",
    "Try to build one composed by two hidden dense layer with ReLU activation and one dense output layer(units=1) with sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8PMtoQ8aY2A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Generate dummy data\n",
    "train_data = np.random.random((1000, 100))\n",
    "train_labels = np.random.randint(2, size=(1000, 1))\n",
    "test_data = np.random.random((100, 100))\n",
    "test_labels = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "# Build your first model by creating a Sequential object and then adding 3 Dense layers:\n",
    "units = 32\n",
    "# Create a Sequential\n",
    "# Add a Dense layer with 32 neurons, with relu as activation function and input dimension equal to the number of features\n",
    "# Add a Dense layer with 32 neurons, with relu as activation function\n",
    "# To produce the output Add a Dense layer with 1 neurons, with sigmoid as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_DJqQnB8ZOP"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "# The fit function output is a History object. The history.history attribute is a record of\n",
    "# training loss values and metrics values at successive epochs, as well as validation loss values \n",
    "# and validation metrics values \n",
    "history = model.fit(train_data, train_labels, epochs=30, batch_size=32)\n",
    "_, train_acc = model.evaluate(train_data, train_labels, verbose=1)\n",
    "_, test_acc = model.evaluate(test_data, test_labels, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8fIyluGaY2E"
   },
   "source": [
    "# 1.3 Build a Deep Neural Network \n",
    "In this section we will:\n",
    "*  1.2.1 Import the dataset\n",
    "*  1.2.2 Build a model\n",
    "*  1.2.3 Train the model \n",
    "*  1.2.4 Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnq7w29FaY2G"
   },
   "source": [
    "## 1.3.1 Import the Dataset\n",
    "Fashion-MNIST is a dataset of Zalando’s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. <br>\n",
    "Each example is a 28×28 grayscale image, associated with a label from 10 classes.<br>\n",
    "<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--s6xGmaZX--/c_imagga_scale,f_auto,fl_progressive,h_900,q_auto,w_1600/https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/doc/img/fashion-mnist-sprite.png\" width=\"600px\"><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LX4Gc3siaY2H"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "mnist_fashion = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_learn, y_learn),(x_test, y_test) = mnist_fashion.load_data()\n",
    "x_learn, x_test = x_learn / 255.0, x_test / 255.0 # normalization \n",
    "x_train, x_val, y_train, y_val = train_test_split(x_learn, y_learn, test_size=0.3, random_state=42) # split learn in train,val\n",
    "num_classes = 10 # Fashion-MNIST classes\n",
    "\n",
    "print(x_train.shape, x_val.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_LsI1CdaY2J"
   },
   "source": [
    "#### Plot some sample from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX44XG1RaY2K"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7Ag6933aY2N"
   },
   "source": [
    "## 1.3.2 Build a Model\n",
    "\n",
    "A Deep Neural Network is a neural network composed by many layers and consequently it has a deeper structure. The number of layers in the network depends on different factors: for example on the data available, on the complexity of the problem, on the computational power and so on.\n",
    "The value produced as output by a neuron is determined by the input the neuron receives and by the activation function. There exists different choices for the activation function. One of the most used is Relu but it depends on the data and on the network architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUrCyhLdaY2R"
   },
   "source": [
    "#### Build a model with this structure: Flatten+Dense(ReLU)+Dense(ReLU)+Dense(ReLU)+Dense(ReLU)+Dense(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jL-UXDSMaY2S"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "# https://keras.io/layers/core/\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "# Add a Dense layer with 512 neurons, with relu as activation function\n",
    "# Add a Dense layer with 256 neurons, with relu as activation function\n",
    "# Add a Dense layer with 128 neurons, with relu as activation function\n",
    "# Add a Dense layer with 64 neurons, with relu as activation function\n",
    "# Add a Dense layer with number of neurons equal to the number of classes, with softmax as activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXOzjck_aY2V"
   },
   "source": [
    "When we are building a model there are many design choises that we must operate: the choice of a Loss Function, the Metrics and the Optimizer.<br>\n",
    "\n",
    "**Loss functions** are used to compare the network's predicted output  with the real output, in each pass of the backpropagations algorithm<br>\n",
    "Common loss functions are: mean-squared error, cross-entropy, and so on...<br><br>\n",
    "**Metrics** are used to evaluate a model; common metrics are precision, recall, accuracy, auc,..<br>\n",
    "\n",
    "The **Optimizer** determines the update rules of the weights. The performance and update speed may heavily vary from optimizer to optimizer; in choosing an optimizer what's important to consider is the network depth, the type of layers and the type of data.<br>\n",
    "The gif below give an idea on how different Optimizers work.<br>\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/11681225/49325458-fc785480-f585-11e8-8d2a-9012d6024c6e.gif\" width=\"460px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01Ms84EmaY2W"
   },
   "source": [
    "#### Configures the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zL2LLmKyaY2W"
   },
   "outputs": [],
   "source": [
    "# Optimizers    https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adadelta, Adagrad, Adamax, Nadam, RMSprop\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "sgd = SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "adad = Adadelta(lr=1.0,rho=0.95,epsilon=None,decay=0.0)\n",
    "adag = Adagrad(lr=0.01,epsilon=None,decay=0.0)\n",
    "adamax = Adamax(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.0)\n",
    "nadam = Nadam(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,schedule_decay=0.004)\n",
    "rms = RMSprop(lr=0.001,rho=0.9,epsilon=None,decay=0.0)\n",
    "\n",
    "# Losses    https://keras.io/losses/\n",
    "loss = ['sparse_categorical_crossentropy','mean_squared_error','mean_absolute_error',\n",
    "        'categorical_crossentropy','categorical_hinge']\n",
    "\n",
    "# Metrics    https://www.tensorflow.org/api_docs/python/tf/metrics\n",
    "metrics = ['accuracy','precision','recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNQcFWhqaY2Z"
   },
   "outputs": [],
   "source": [
    "# Compile the model you created before using \n",
    "# adam optimizer as optimizer\n",
    "# sparse categorical crossentropy as loss function\n",
    "# accuracy as metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SvR02wtaY2c"
   },
   "source": [
    "## 1.3.3 Train the model \n",
    "The batch size is a number of samples processed before the model is updated.<br>\n",
    "The number of epochs is the number of complete passes through the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wd4Q9bFjaY2c"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "# fit your model and save the returned value as \"history\". \n",
    "# Use both the train and validation set \n",
    "# Set both properly the batch size value and the epochs value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv73zOpQaY2f"
   },
   "source": [
    "#### Training history visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuYCcmXGaY2h"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXWlXkuBaY2t",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqEKGdT6aY2v"
   },
   "source": [
    "**What could you notice in the loss graph training the model over large number of epochs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8zJr0nraY2w"
   },
   "source": [
    "## 1.3.4 Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lxASSVmaY2x"
   },
   "outputs": [],
   "source": [
    "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2stTI0NaY2z"
   },
   "source": [
    "**Try to play with these parameters (loss and optimizers) in order to see how this choice affects the accuracy. What do you expect? which is faster?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znWL73IWaY20"
   },
   "source": [
    "# 1.4 Dealing with overfitting\n",
    "Given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.<br>\n",
    "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether).<br>\n",
    "How to improve generalization of our model on unseen data?<br>\n",
    "In this section we will:\n",
    "* 1.3.1 Add weight regularization\n",
    "* 1.3.2 Dropout\n",
    "* 1.3.3 Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aimRiLIqaY22"
   },
   "source": [
    "## 1.4.1 Add weight regularization\n",
    "A common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\".<br>\n",
    "This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights.<br> This cost comes in two flavors:\n",
    "* L1 regularization\n",
    "* L2 regularization\n",
    "\n",
    "In tf.keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vE2zlBgeaY23",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from  tensorflow.keras import regularizers\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "# Add a Dense layer with 512 neurons, with relu as activation function and a l2 regularizer (with 0.001 as regularization value) as kernel regularizer \n",
    "# Add a Dense layer with 256 neurons, with relu as activation function and a l2 regularizer (with 0.001 as regularization value) as kernel regularizer \n",
    "# Add a Dense layer with 128 neurons, with relu as activation function and a l2 regularizer (with 0.001 as regularization value) as kernel regularizer \n",
    "# Add a Dense layer with 64 neurons, with relu as activation function and a l2 regularizer (with 0.001 as regularization value) as kernel regularizer \n",
    "# Add a Dense layer with number of neurons equal to the number of classes, with softmax as activation function\n",
    "\n",
    "\n",
    "# Compile the model you just created using \n",
    "# adam optimizer as optimizer\n",
    "# sparse categorical crossentropy as loss function\n",
    "# accuracy as metric\n",
    "\n",
    "\n",
    "# fit your model and save the returned value as \"history\". \n",
    "# Use both the train and validation set \n",
    "# Set both properly the batch size value and the epochs value\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# Call the plot_history function to plot the obtained results\n",
    "\n",
    "# Evaluate\n",
    "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4xF5NBSaY25"
   },
   "source": [
    "## 1.4.2 Dropout\n",
    "Dropout is one of the most effective and most commonly used regularization techniques for neural networks.<br>\n",
    "Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. set to zero) a number of output features of the layer during training.<br>\n",
    "\n",
    "The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5; at test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzV7zpjJaY26",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "# Add a Dense layer with 512 neurons, with relu as activation function\n",
    "# Add a Dropout layer with 0.3 drop probability\n",
    "# Add a Dense layer with 256 neurons, with relu as activation function\n",
    "# Add a Dropout layer with 0.3 drop probability\n",
    "# Add a Dense layer with 128 neurons, with relu as activation function\n",
    "# Add a Dropout layer with 0.3 drop probability\n",
    "# Add a Dense layer with 64 neurons, with relu as activation function\n",
    "# Add a Dense layer with number of neurons equal to the number of classes, with softmax as activation function\n",
    "\n",
    "\n",
    "# Compile the model you just created using \n",
    "# adam optimizer as optimizer\n",
    "# sparse categorical crossentropy as loss function\n",
    "# accuracy as metric\n",
    "\n",
    "\n",
    "# fit your model and save the returned value as \"history\". \n",
    "# Use both the train and validation set \n",
    "# Set both properly the batch size value and the epochs value\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# Call the plot_history function to plot the obtained results\n",
    "\n",
    "# Evaluate\n",
    "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5_Yf-4UaY29"
   },
   "source": [
    "## 1.4.3 Early stopping\n",
    "Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcJ9tjH2aY2-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "# early stopping https://keras.io/callbacks/\n",
    "es_callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "# Create checkpoint callback that will save the best model observed during training for later use\n",
    "checkpoint_path = \"output/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = ModelCheckpoint(checkpoint_path,monitor='val_loss',save_weights_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAaj7qisaY3B",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "# Add a Dense layer with 512 neurons, with relu as activation function\n",
    "# Add a Dense layer with 256 neurons, with relu as activation function\n",
    "# Add a Dense layer with 128 neurons, with relu as activation function\n",
    "# Add a Dense layer with 64 neurons, with relu as activation function\n",
    "# Add a Dense layer with number of neurons equal to the number of classes, with softmax as activation function\n",
    "\n",
    "\n",
    "# Compile the model you just created using \n",
    "# adam optimizer as optimizer\n",
    "# sparse categorical crossentropy as loss function\n",
    "# accuracy as metric\n",
    "\n",
    "\n",
    "# fit your model and save the returned value as \"history\". \n",
    "# Use both the train and validation set \n",
    "# Set both properly the batch size value and the epochs value\n",
    "# Be careful to also set properly the callbacks parameter list\n",
    "batch_size =128\n",
    "epochs = 10\n",
    "\n",
    "# Call the plot_history function to plot the obtained results\n",
    "\n",
    "# Evaluate\n",
    "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIT021CHaY3D"
   },
   "source": [
    "#### Load weights\n",
    "The saved weights can then be loaded and evaluated any time by calling the load_weights() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WfKJE7NaY3D",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored.\n",
    "# link https://www.tensorflow.org/tutorials/keras/save_and_restore_models\n",
    "\n",
    "checkpoint_path = \"output/cp.ckpt\"\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLP_DNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
